{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL engine created successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Import Libraries and Set Up MySQL Connection\n",
    "#\n",
    "# Import the required libraries and set up the MySQL connection using SQLAlchemy.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection details\n",
    "username = 'root'\n",
    "password = '12345'\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "database = 'case3'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "print(\"MySQL engine created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Data Shape: (1000, 12)\n",
      "Attrition Data Shape: (144, 3)\n",
      "Performance Data Shape: (1000, 6)\n",
      "Columns in employee_df after dropping 'gender': ['Employee_ID', 'Age', 'first_name', 'last_name', 'Gender', 'Department', 'Job_Role', 'Education_Level', 'Marital_Status', 'Job_Tenure', 'Distance_From_Home']\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Read CSV Files\n",
    "#\n",
    "# Load the three CSV files: Employee Data, Attrition Data, and Employee Performance Data.\n",
    "# Note: In the employee data, we ignore the lower-case 'gender' column and keep the 'Gender' column.\n",
    "\n",
    "# Read CSV files\n",
    "employee_df = pd.read_csv('employee_data 1.csv')\n",
    "attrition_df = pd.read_csv('Attrition 1.csv')\n",
    "performance_df = pd.read_csv('employee_performance_data 1.csv')\n",
    "\n",
    "# Display initial shapes\n",
    "print(\"Employee Data Shape:\", employee_df.shape)\n",
    "print(\"Attrition Data Shape:\", attrition_df.shape)\n",
    "print(\"Performance Data Shape:\", performance_df.shape)\n",
    "\n",
    "# Drop lower-case 'gender' column if it exists; keep only the 'Gender' column.\n",
    "if 'gender' in employee_df.columns:\n",
    "    employee_df = employee_df.drop(columns=['gender'])\n",
    "print(\"Columns in employee_df after dropping 'gender':\", employee_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee_ID is unique in employee_data.\n",
      "Employee_ID has duplicates in attrition_data.\n",
      "Employee_ID is unique in performance_data.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Validate Primary Key Uniqueness\n",
    "#\n",
    "# Ensure that Employee_ID is unique in each dataset for data integrity.\n",
    "\n",
    "# Check uniqueness in employee data\n",
    "if employee_df['Employee_ID'].is_unique:\n",
    "    print(\"Employee_ID is unique in employee_data.\")\n",
    "else:\n",
    "    print(\"Employee_ID has duplicates in employee_data.\")\n",
    "\n",
    "# Standardize column name in attrition data and check uniqueness\n",
    "attrition_df.rename(columns={'employee_ID': 'Employee_ID'}, inplace=True)\n",
    "if attrition_df['Employee_ID'].is_unique:\n",
    "    print(\"Employee_ID is unique in attrition_data.\")\n",
    "else:\n",
    "    print(\"Employee_ID has duplicates in attrition_data.\")\n",
    "\n",
    "# Check uniqueness in performance data\n",
    "if performance_df['Employee_ID'].is_unique:\n",
    "    print(\"Employee_ID is unique in performance_data.\")\n",
    "else:\n",
    "    print(\"Employee_ID has duplicates in performance_data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging employee and performance data: (1000, 16)\n",
      "Shape before dropping incomplete records: (1017, 18)\n",
      "Shape after dropping rows with missing attrition or exit interview score: (144, 18)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Merge Datasets and Remove Incomplete Records\n",
    "#\n",
    "# Merge the employee and performance data first, then join with attrition data.\n",
    "# Finally, drop rows missing either `attrition` or `Exit_Interview_Score` since these records arenâ€™t useful for analysis.\n",
    "\n",
    "# Merge employee and performance data on Employee_ID (inner join)\n",
    "emp_perf_df = pd.merge(employee_df, performance_df, on='Employee_ID', how='inner')\n",
    "print(\"Shape after merging employee and performance data:\", emp_perf_df.shape)\n",
    "\n",
    "# Merge with attrition data (left join)\n",
    "full_df = pd.merge(emp_perf_df, attrition_df, on='Employee_ID', how='left')\n",
    "print(\"Shape before dropping incomplete records:\", full_df.shape)\n",
    "\n",
    "# Drop rows with missing values for 'attrition' or 'Exit_Interview_Score'\n",
    "full_df = full_df.dropna(subset=['attrition', 'Exit_Interview_Score'])\n",
    "print(\"Shape after dropping rows with missing attrition or exit interview score:\", full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Dimension Shape: (127, 8)\n",
      "Department Dimension Shape: (6, 2)\n",
      "Role Dimension Shape: (6, 2)\n",
      "Fact Table Shape: (178, 10)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Build Star Schema without dim_time and with Separate Department/Role Dimensions\n",
    "#\n",
    "# Create the fact table and dimension tables. The changes include:\n",
    "#\n",
    "# - **Fact Table (`fact_employee_performance`):** Contains performance metrics, attrition, exit interview score, and references to department and role via their surrogate keys.\n",
    "# - **Employee Dimension (`dim_employee`):** Contains personal attributes without department or job role.\n",
    "# - **Department Dimension (`dim_department`):** Contains unique departments with a surrogate key.\n",
    "# - **Role Dimension (`dim_role`):** Contains unique job roles with a surrogate key.\n",
    "#\n",
    "# The fact table is updated to merge department and role IDs from their respective dimension tables.\n",
    "\n",
    "# Create Fact Table with performance metrics and attrition data\n",
    "fact_table = full_df[['Employee_ID', 'Performance_Rating', 'Last_Promotion_Year', \n",
    "                        'Training_Hours', 'Work_Life_Balance', 'Job_Satisfaction', \n",
    "                        'attrition', 'Exit_Interview_Score']].copy()\n",
    "\n",
    "# Create Employee Dimension (exclude department and job role)\n",
    "dim_employee = full_df[['Employee_ID', 'Age', 'first_name', 'last_name', 'Gender', \n",
    "                          'Education_Level', 'Marital_Status', 'Job_Tenure', 'Distance_From_Home']].drop_duplicates()\n",
    "\n",
    "# Merge first and last names into a single column 'name'\n",
    "dim_employee['Name'] = dim_employee['first_name'] + ' ' + dim_employee['last_name']\n",
    "dim_employee = dim_employee.drop(columns=['first_name', 'last_name'])\n",
    "\n",
    "print(\"Employee Dimension Shape:\", dim_employee.shape)\n",
    "\n",
    "# Create Department Dimension: Unique departments with surrogate key\n",
    "dim_department = full_df[['Department']].drop_duplicates().reset_index(drop=True)\n",
    "dim_department['Department_ID'] = dim_department.index + 1\n",
    "print(\"Department Dimension Shape:\", dim_department.shape)\n",
    "\n",
    "# Create Role Dimension: Unique job roles with surrogate key\n",
    "dim_role = full_df[['Job_Role']].drop_duplicates().reset_index(drop=True)\n",
    "dim_role['Role_ID'] = dim_role.index + 1\n",
    "print(\"Role Dimension Shape:\", dim_role.shape)\n",
    "\n",
    "# Merge Department and Role info into fact table:\n",
    "# First, add the original department and job role columns to fact table for the lookup.\n",
    "fact_table = pd.merge(fact_table, full_df[['Employee_ID', 'Department', 'Job_Role']], on='Employee_ID', how='left')\n",
    "\n",
    "# Merge department ID from dim_department\n",
    "fact_table = pd.merge(fact_table, dim_department, on='Department', how='left')\n",
    "\n",
    "# Merge role ID from dim_role\n",
    "fact_table = pd.merge(fact_table, dim_role, on='Job_Role', how='left')\n",
    "\n",
    "# Remove redundant text columns (Department and Job_Role) after merging IDs\n",
    "fact_table.drop(columns=['Department', 'Job_Role'], inplace=True)\n",
    "\n",
    "print(\"Fact Table Shape:\", fact_table.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to MySQL successfully.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Load Tables to MySQL\n",
    "#\n",
    "# Finally, load the fact and dimension tables into MySQL.\n",
    "# Table names are converted to lower-case during the load process.\n",
    "\n",
    "# Load tables into MySQL\n",
    "fact_table.to_sql('fact_employee_performance', con=engine, if_exists='replace', index=False)\n",
    "dim_employee.to_sql('dim_employee', con=engine, if_exists='replace', index=False)\n",
    "dim_department.to_sql('dim_department', con=engine, if_exists='replace', index=False)\n",
    "dim_role.to_sql('dim_role', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Data loaded to MySQL successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
