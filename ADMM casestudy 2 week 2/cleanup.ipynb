{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Chain ETL with Star Schema (Updated)\n",
    "\n",
    "This notebook/script:\n",
    "\n",
    "1. **Reads** the CSV data for Sales, Inventory, Suppliers, and Purchase Orders.\n",
    "2. **Cleans** and **validates** data:\n",
    "   - Checks for missing values in primary key fields.\n",
    "   - Ensures uniqueness for single-column primary keys (Sale_ID, Supplier_ID, Order_ID).\n",
    "   - Resolves duplicates in Inventory (composite key of Product_ID, Store_ID, Warehouse_ID) by keeping only the latest Last_Updated record.\n",
    "3. **Constructs a Star Schema**:\n",
    "   - **Dimension Tables**: \n",
    "     - dim_products\n",
    "     - dim_suppliers\n",
    "     - dim_stores\n",
    "     - dim_warehouses\n",
    "     - dim_dates\n",
    "   - **Fact Tables**: \n",
    "     - fact_sales\n",
    "     - fact_inventory\n",
    "     - fact_purchase_orders\n",
    "4. **Loads** these dimension and fact tables into MySQL using SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# =============== 1. Extraction ===============\n",
    "\n",
    "sales_file = \"sales_data-2.csv\"\n",
    "inventory_file = \"inventory_data.csv\"\n",
    "suppliers_file = \"suppliers_data.csv\"\n",
    "purchase_orders_file = \"purchase_orders_data.csv\"\n",
    "\n",
    "# Read CSV files\n",
    "sales_df = pd.read_csv(sales_file, parse_dates=[\"Sale_Date\"])\n",
    "inventory_df = pd.read_csv(inventory_file, parse_dates=[\"Last_Updated\"])\n",
    "suppliers_df = pd.read_csv(suppliers_file)\n",
    "purchase_orders_df = pd.read_csv(purchase_orders_file, parse_dates=[\"Order_Date\", \"Arrival_Date\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Sales Data:\n",
      " Sale_ID          0\n",
      "Product_ID       0\n",
      "Store_ID         0\n",
      "Sale_Date        0\n",
      "Quantity_Sold    0\n",
      "Revenue          0\n",
      "dtype: int64 \n",
      "\n",
      "Missing values in Inventory Data:\n",
      " Product_ID       0\n",
      "Store_ID         0\n",
      "Warehouse_ID     0\n",
      "Stock_Level      0\n",
      "Reorder_Level    0\n",
      "Last_Updated     0\n",
      "dtype: int64 \n",
      "\n",
      "Missing values in Suppliers Data:\n",
      " Supplier_ID         0\n",
      "Supplier_Name       0\n",
      "Product_ID          0\n",
      "Lead_Time (days)    0\n",
      "Order_Frequency     0\n",
      "dtype: int64 \n",
      "\n",
      "Missing values in Purchase Orders Data:\n",
      " Order_ID        0\n",
      "Product_ID      0\n",
      "Supplier_ID     0\n",
      "Order_Date      0\n",
      "Quantity        0\n",
      "Arrival_Date    0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2.1 Check for missing values in critical columns ---\n",
    "print(\"Missing values in Sales Data:\\n\", sales_df.isnull().sum(), \"\\n\")\n",
    "print(\"Missing values in Inventory Data:\\n\", inventory_df.isnull().sum(), \"\\n\")\n",
    "print(\"Missing values in Suppliers Data:\\n\", suppliers_df.isnull().sum(), \"\\n\")\n",
    "print(\"Missing values in Purchase Orders Data:\\n\", purchase_orders_df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Drop rows missing primary key fields\n",
    "sales_df.dropna(subset=[\"Sale_ID\"], inplace=True)\n",
    "suppliers_df.dropna(subset=[\"Supplier_ID\"], inplace=True)\n",
    "purchase_orders_df.dropna(subset=[\"Order_ID\"], inplace=True)\n",
    "inventory_df.dropna(subset=[\"Product_ID\", \"Store_ID\", \"Warehouse_ID\"], inplace=True)\n",
    "\n",
    "# --- 2.2 Ensure single-column primary keys are unique (Sales, Suppliers, Purchase Orders) ---\n",
    "if sales_df[\"Sale_ID\"].duplicated().any():\n",
    "    print(\"Duplicate Sale_ID found. Keeping first occurrence.\")\n",
    "    sales_df.drop_duplicates(subset=[\"Sale_ID\"], keep=\"first\", inplace=True)\n",
    "\n",
    "if suppliers_df[\"Supplier_ID\"].duplicated().any():\n",
    "    print(\"Duplicate Supplier_ID found. Keeping first occurrence.\")\n",
    "    suppliers_df.drop_duplicates(subset=[\"Supplier_ID\"], keep=\"first\", inplace=True)\n",
    "\n",
    "if purchase_orders_df[\"Order_ID\"].duplicated().any():\n",
    "    print(\"Duplicate Order_ID found. Keeping first occurrence.\")\n",
    "    purchase_orders_df.drop_duplicates(subset=[\"Order_ID\"], keep=\"first\", inplace=True)\n",
    "\n",
    "# --- 2.3 Resolve composite key duplicates in Inventory ---\n",
    "# (Product_ID, Store_ID, Warehouse_ID) must be unique; keep the latest Last_Updated\n",
    "inventory_df.sort_values(by=\"Last_Updated\", ascending=False, inplace=True)\n",
    "inventory_df.drop_duplicates(subset=[\"Product_ID\", \"Store_ID\", \"Warehouse_ID\"], keep=\"first\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Star Schema Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Dimension: dim_products ---\n",
    "# Collect unique Product_ID from all tables referencing products\n",
    "all_product_ids = set(sales_df[\"Product_ID\"].dropna().unique()) \\\n",
    "    .union(inventory_df[\"Product_ID\"].dropna().unique()) \\\n",
    "    .union(purchase_orders_df[\"Product_ID\"].dropna().unique()) \\\n",
    "    .union(suppliers_df[\"Product_ID\"].dropna().unique())\n",
    "\n",
    "dim_products = pd.DataFrame({\"Product_ID\": sorted(all_product_ids)})\n",
    "dim_products[\"product_key\"] = range(1, len(dim_products) + 1)\n",
    "dim_products = dim_products[[\"product_key\", \"Product_ID\"]].copy()\n",
    "\n",
    "# --- 3.2 Dimension: dim_suppliers ---\n",
    "# Group by Supplier_ID to ensure one row per supplier\n",
    "suppliers_gb = suppliers_df.groupby(\"Supplier_ID\", as_index=False).agg({\n",
    "    \"Supplier_Name\": \"first\",\n",
    "    \"Lead_Time (days)\": \"first\",\n",
    "    \"Order_Frequency\": \"first\"\n",
    "})\n",
    "dim_suppliers = suppliers_gb.copy()\n",
    "dim_suppliers[\"supplier_key\"] = range(1, len(dim_suppliers) + 1)\n",
    "dim_suppliers = dim_suppliers[[\n",
    "    \"supplier_key\",\n",
    "    \"Supplier_ID\",\n",
    "    \"Supplier_Name\",\n",
    "    \"Lead_Time (days)\",\n",
    "    \"Order_Frequency\"\n",
    "]].copy()\n",
    "\n",
    "# --- 3.3 Dimension: dim_stores ---\n",
    "# Gather unique Store_IDs from sales and inventory\n",
    "all_store_ids = set(sales_df[\"Store_ID\"].dropna().unique()) \\\n",
    "    .union(inventory_df[\"Store_ID\"].dropna().unique())\n",
    "\n",
    "dim_stores = pd.DataFrame({\"Store_ID\": sorted(all_store_ids)})\n",
    "dim_stores[\"store_key\"] = range(1, len(dim_stores) + 1)\n",
    "dim_stores = dim_stores[[\"store_key\", \"Store_ID\"]].copy()\n",
    "\n",
    "# --- 3.4 Dimension: dim_warehouses ---\n",
    "# Gather unique Warehouse_ID from inventory\n",
    "all_warehouse_ids = set(inventory_df[\"Warehouse_ID\"].dropna().unique())\n",
    "dim_warehouses = pd.DataFrame({\"Warehouse_ID\": sorted(all_warehouse_ids)})\n",
    "dim_warehouses[\"warehouse_key\"] = range(1, len(dim_warehouses) + 1)\n",
    "dim_warehouses = dim_warehouses[[\"warehouse_key\", \"Warehouse_ID\"]].copy()\n",
    "\n",
    "# --- 3.5 Dimension: dim_dates ---\n",
    "# Collect all date columns: Sale_Date, Last_Updated, Order_Date, Arrival_Date\n",
    "dates_sales = sales_df[\"Sale_Date\"].dropna().unique()\n",
    "dates_inv = inventory_df[\"Last_Updated\"].dropna().unique()\n",
    "dates_po_order = purchase_orders_df[\"Order_Date\"].dropna().unique()\n",
    "dates_po_arrival = purchase_orders_df[\"Arrival_Date\"].dropna().unique()\n",
    "\n",
    "all_dates = pd.Series(list(dates_sales) + list(dates_inv) + list(dates_po_order) + list(dates_po_arrival)).unique()\n",
    "all_dates = pd.to_datetime(all_dates)\n",
    "all_dates = sorted(all_dates)\n",
    "\n",
    "dim_dates = pd.DataFrame({\"date\": all_dates})\n",
    "dim_dates[\"date_key\"] = range(1, len(dim_dates) + 1)\n",
    "dim_dates[\"year\"] = dim_dates[\"date\"].dt.year\n",
    "dim_dates[\"month\"] = dim_dates[\"date\"].dt.month\n",
    "dim_dates[\"day\"] = dim_dates[\"date\"].dt.day\n",
    "dim_dates = dim_dates[[\"date_key\", \"date\", \"year\", \"month\", \"day\"]].copy()\n",
    "\n",
    "# =============== Helper Functions for Surrogate Key Mapping ===============\n",
    "\n",
    "def map_product_key(df, product_id_col):\n",
    "    return pd.merge(\n",
    "        df, \n",
    "        dim_products, \n",
    "        how=\"left\", \n",
    "        left_on=product_id_col, \n",
    "        right_on=\"Product_ID\"\n",
    "    )\n",
    "\n",
    "def map_supplier_key(df, supplier_id_col):\n",
    "    return pd.merge(\n",
    "        df,\n",
    "        dim_suppliers,\n",
    "        how=\"left\",\n",
    "        left_on=supplier_id_col,\n",
    "        right_on=\"Supplier_ID\"\n",
    "    )\n",
    "\n",
    "def map_store_key(df, store_id_col):\n",
    "    return pd.merge(\n",
    "        df,\n",
    "        dim_stores,\n",
    "        how=\"left\",\n",
    "        left_on=store_id_col,\n",
    "        right_on=\"Store_ID\"\n",
    "    )\n",
    "\n",
    "def map_warehouse_key(df, warehouse_id_col):\n",
    "    return pd.merge(\n",
    "        df,\n",
    "        dim_warehouses,\n",
    "        how=\"left\",\n",
    "        left_on=warehouse_id_col,\n",
    "        right_on=\"Warehouse_ID\"\n",
    "    )\n",
    "\n",
    "def map_date_key(df, date_col):\n",
    "    # merges on exact match of date\n",
    "    return pd.merge(\n",
    "        df,\n",
    "        dim_dates,\n",
    "        how=\"left\",\n",
    "        left_on=date_col,\n",
    "        right_on=\"date\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 fact_sales ---\n",
    "# Original columns: [Sale_ID, Product_ID, Store_ID, Sale_Date, Quantity_Sold, Revenue]\n",
    "fact_sales = sales_df.copy()\n",
    "\n",
    "fact_sales = map_product_key(fact_sales, \"Product_ID\")\n",
    "fact_sales = map_store_key(fact_sales, \"Store_ID\")\n",
    "fact_sales = map_date_key(fact_sales, \"Sale_Date\")\n",
    "\n",
    "fact_sales = fact_sales[[\n",
    "    \"Sale_ID\",\n",
    "    \"product_key\",\n",
    "    \"store_key\",\n",
    "    \"date_key\",\n",
    "    \"Quantity_Sold\",\n",
    "    \"Revenue\"\n",
    "]].copy()\n",
    "\n",
    "# --- 4.2 fact_inventory ---\n",
    "# Original columns: [Product_ID, Store_ID, Warehouse_ID, Stock_Level, Reorder_Level, Last_Updated]\n",
    "fact_inventory = inventory_df.copy()\n",
    "\n",
    "fact_inventory = map_product_key(fact_inventory, \"Product_ID\")\n",
    "fact_inventory = map_store_key(fact_inventory, \"Store_ID\")\n",
    "fact_inventory = map_warehouse_key(fact_inventory, \"Warehouse_ID\")\n",
    "fact_inventory = map_date_key(fact_inventory, \"Last_Updated\")\n",
    "\n",
    "fact_inventory = fact_inventory[[\n",
    "    \"product_key\",\n",
    "    \"store_key\",\n",
    "    \"warehouse_key\",\n",
    "    \"Stock_Level\",\n",
    "    \"Reorder_Level\",\n",
    "    \"date_key\"  # or rename to 'last_updated_date_key'\n",
    "]].copy()\n",
    "\n",
    "# --- 4.3 fact_purchase_orders ---\n",
    "# Original columns: [Order_ID, Product_ID, Supplier_ID, Order_Date, Quantity, Arrival_Date]\n",
    "fact_purchase_orders = purchase_orders_df.copy()\n",
    "\n",
    "# Map product_key & supplier_key\n",
    "fact_purchase_orders = map_product_key(fact_purchase_orders, \"Product_ID\")\n",
    "fact_purchase_orders = map_supplier_key(fact_purchase_orders, \"Supplier_ID\")\n",
    "\n",
    "# Map order_date_key\n",
    "fact_purchase_orders = map_date_key(fact_purchase_orders, \"Order_Date\")\n",
    "fact_purchase_orders.rename(columns={\"date_key\": \"order_date_key\"}, inplace=True)\n",
    "\n",
    "# Map arrival_date_key\n",
    "fact_purchase_orders = map_date_key(fact_purchase_orders, \"Arrival_Date\")\n",
    "fact_purchase_orders.rename(columns={\"date_key\": \"arrival_date_key\"}, inplace=True)\n",
    "\n",
    "fact_purchase_orders = fact_purchase_orders[[\n",
    "    \"Order_ID\",\n",
    "    \"product_key\",\n",
    "    \"supplier_key\",\n",
    "    \"order_date_key\",\n",
    "    \"Quantity\",\n",
    "    \"arrival_date_key\"\n",
    "]].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load into MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Schema tables loaded successfully into MySQL.\n"
     ]
    }
   ],
   "source": [
    "# Update credentials and DB name as needed\n",
    "username = 'root'\n",
    "password = '12345'\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "database = 'case4'\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{host}:{port}/{database}\")\n",
    "\n",
    "# --- 5.1 Write dimension tables ---\n",
    "dim_products.to_sql(\"dim_products\", con=engine, if_exists=\"replace\", index=False)\n",
    "dim_suppliers.to_sql(\"dim_suppliers\", con=engine, if_exists=\"replace\", index=False)\n",
    "dim_stores.to_sql(\"dim_stores\", con=engine, if_exists=\"replace\", index=False)\n",
    "dim_warehouses.to_sql(\"dim_warehouses\", con=engine, if_exists=\"replace\", index=False)\n",
    "dim_dates.to_sql(\"dim_dates\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# --- 5.2 Write fact tables ---\n",
    "fact_sales.to_sql(\"fact_sales\", con=engine, if_exists=\"replace\", index=False)\n",
    "fact_inventory.to_sql(\"fact_inventory\", con=engine, if_exists=\"replace\", index=False)\n",
    "fact_purchase_orders.to_sql(\"fact_purchase_orders\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"Star Schema tables loaded successfully into MySQL.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
