{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sensor_id values are unique in sensor data.\n",
      "All accident_id values are unique in accident data.\n",
      "Missing values in sensor data:\n",
      "sensor_id           0\n",
      "location            0\n",
      "date_time           0\n",
      "vehicle_count       0\n",
      "average_speed       0\n",
      "congestion_level    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in accident data:\n",
      "accident_id           0\n",
      "date_time             0\n",
      "location              0\n",
      "weather_condition     0\n",
      "road_condition        0\n",
      "vehicle_type          0\n",
      "accident_severity     0\n",
      "number_of_vehicles    0\n",
      "casualties            0\n",
      "traffic_density       0\n",
      "dtype: int64\n",
      "dim_time:\n",
      "   time_id           date_time  year  month  day  hour  day_of_week\n",
      "0        1 2024-01-01 00:00:00  2024      1    1     0            0\n",
      "1        2 2024-01-01 01:00:00  2024      1    1     1            0\n",
      "2        3 2024-01-01 02:00:00  2024      1    1     2            0\n",
      "3        4 2024-01-01 03:00:00  2024      1    1     3            0\n",
      "4        5 2024-01-01 04:00:00  2024      1    1     4            0\n",
      "\n",
      "dim_location:\n",
      "   location_id          location\n",
      "0            1          Downtown\n",
      "1            2           Highway\n",
      "2            3  Residential Zone\n",
      "3            4   Industrial Area\n",
      "4            5           Suburbs\n",
      "\n",
      "dim_vehicle:\n",
      "   vehicle_id vehicle_type\n",
      "0           1          Bus\n",
      "1           2   Motorcycle\n",
      "2           3      Bicycle\n",
      "3           4          Car\n",
      "4           5        Truck\n",
      "\n",
      "dim_weather:\n",
      "   weather_id weather_condition\n",
      "0           1               Fog\n",
      "1           2             Clear\n",
      "2           3              Snow\n",
      "3           4             Storm\n",
      "4           5              Rain\n",
      "\n",
      "dim_road:\n",
      "   road_id      road_condition\n",
      "0        1                 Icy\n",
      "1        2                 Dry\n",
      "2        3  Under Construction\n",
      "3        4                 Wet\n",
      "\n",
      "fact_traffic:\n",
      "  sensor_id  fk_time_id  fk_location_id  vehicle_count  average_speed  \\\n",
      "0    a36f14           1               1             90             59   \n",
      "1    256a39           2               1            324             73   \n",
      "2    a8a01f           3               2             61             56   \n",
      "3    3036e3           4               2             90             56   \n",
      "4    2b45b1           5               3             72             77   \n",
      "\n",
      "  congestion_level  \n",
      "0             High  \n",
      "1         Moderate  \n",
      "2         Moderate  \n",
      "3             High  \n",
      "4             High  \n",
      "\n",
      "fact_accident:\n",
      "  accident_id  fk_time_id  fk_location_id  fk_vehicle_id  fk_weather_id  \\\n",
      "0    d02b3836           1               5              1              1   \n",
      "1    694c8577           2               1              1              2   \n",
      "2    e0ed5c55           3               3              2              3   \n",
      "3    6a78511a           4               1              3              1   \n",
      "4    547bf4c2           5               2              1              4   \n",
      "\n",
      "   fk_road_id accident_severity  number_of_vehicles  casualties  \\\n",
      "0           1             Minor                   2           6   \n",
      "1           1             Minor                   2           2   \n",
      "2           2          Moderate                   4           5   \n",
      "3           1          Moderate                   3           8   \n",
      "4           3          Moderate                   4           1   \n",
      "\n",
      "  traffic_density  \n",
      "0             Low  \n",
      "1             Low  \n",
      "2        Moderate  \n",
      "3        Moderate  \n",
      "4            High  \n",
      "\n",
      "All tables have been loaded successfully into MySQL!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Smart City Traffic & Accident Analytics ETL Process with dim_time\n",
    "#\n",
    "# This notebook:\n",
    "# 1. Loads sensor and accident CSV data.\n",
    "# 2. Performs data quality checks (uniqueness of primary keys and missing data).\n",
    "# 3. Cleans and transforms the data (e.g., converting date columns, standardizing column names).\n",
    "# 4. Creates dimension tables:\n",
    "#    - dim_time\n",
    "#    - dim_location\n",
    "#    - dim_vehicle\n",
    "#    - dim_weather\n",
    "#    - dim_road\n",
    "# 5. Creates fact tables:\n",
    "#    - fact_traffic\n",
    "#    - fact_accident\n",
    "# 6. Loads the resulting tables into a MySQL database using SQLAlchemy.\n",
    "#\n",
    "# **Note**: The MySQL connection details are:\n",
    "#\n",
    "# username = 'root'\n",
    "# password = '12345'\n",
    "# host = 'localhost'\n",
    "# port = '3306'\n",
    "# database = 'case1'\n",
    "#\n",
    "# Table names are created in lower-case.\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Load CSV Files\n",
    "#\n",
    "# Read the CSV files into pandas DataFrames and standardize column names.\n",
    "\n",
    "# %% [code]\n",
    "# Load CSV files\n",
    "df_sensor = pd.read_csv('road_traffic_sensor_data.csv')\n",
    "df_accident = pd.read_csv('traffic_accident_data.csv')\n",
    "\n",
    "# Standardize column names to lower-case\n",
    "df_sensor.columns = [col.lower() for col in df_sensor.columns]\n",
    "df_accident.columns = [col.lower() for col in df_accident.columns]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Quality Checks & Cleaning\n",
    "#\n",
    "# ### 2.1 Check for Uniqueness of Primary Keys\n",
    "\n",
    "# %% [code]\n",
    "# For sensor data, the primary key is sensor_id\n",
    "if df_sensor['sensor_id'].nunique() != len(df_sensor):\n",
    "    print(\"Warning: Duplicate sensor_id values found in sensor data!\")\n",
    "else:\n",
    "    print(\"All sensor_id values are unique in sensor data.\")\n",
    "\n",
    "# For accident data, the primary key is accident_id\n",
    "if df_accident['accident_id'].nunique() != len(df_accident):\n",
    "    print(\"Warning: Duplicate accident_id values found in accident data!\")\n",
    "else:\n",
    "    print(\"All accident_id values are unique in accident data.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.2 Check for Missing Data\n",
    "\n",
    "# %% [code]\n",
    "print(\"Missing values in sensor data:\")\n",
    "print(df_sensor.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in accident data:\")\n",
    "print(df_accident.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values (or alternatively, you can impute them)\n",
    "df_sensor.dropna(inplace=True)\n",
    "df_accident.dropna(inplace=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.3 Convert Data Types\n",
    "#\n",
    "# Convert the **date_time** columns to datetime objects.\n",
    "\n",
    "# %% [code]\n",
    "df_sensor['date_time'] = pd.to_datetime(df_sensor['date_time'])\n",
    "df_accident['date_time'] = pd.to_datetime(df_accident['date_time'])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Build Dimension Tables\n",
    "#\n",
    "# ### 3.1 dim_time\n",
    "#\n",
    "# We combine unique date_time values from both sensor and accident datasets.  \n",
    "# Then we extract additional time attributes (year, month, day, hour, etc.).\n",
    "\n",
    "# %% [code]\n",
    "# Combine unique date_time values from sensor and accident data\n",
    "all_times = pd.concat([df_sensor[['date_time']], df_accident[['date_time']]])\n",
    "all_times = all_times.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Extract time attributes\n",
    "all_times['year'] = all_times['date_time'].dt.year\n",
    "all_times['month'] = all_times['date_time'].dt.month\n",
    "all_times['day'] = all_times['date_time'].dt.day\n",
    "all_times['hour'] = all_times['date_time'].dt.hour\n",
    "all_times['day_of_week'] = all_times['date_time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Create a surrogate key for time_id (starting at 1)\n",
    "all_times.reset_index(inplace=True)\n",
    "all_times.rename(columns={'index': 'time_id'}, inplace=True)\n",
    "all_times['time_id'] = all_times['time_id'] + 1\n",
    "\n",
    "dim_time = all_times[['time_id', 'date_time', 'year', 'month', 'day', 'hour', 'day_of_week']]\n",
    "print(\"dim_time:\")\n",
    "print(dim_time.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.2 dim_location\n",
    "#\n",
    "# Consolidate distinct locations from both datasets.\n",
    "\n",
    "# %% [code]\n",
    "locations_sensor = df_sensor[['location']].drop_duplicates()\n",
    "locations_accident = df_accident[['location']].drop_duplicates()\n",
    "all_locations = pd.concat([locations_sensor, locations_accident]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Surrogate key for location_id (starting at 1)\n",
    "all_locations.reset_index(inplace=True)\n",
    "all_locations.rename(columns={'index': 'location_id'}, inplace=True)\n",
    "all_locations['location_id'] = all_locations['location_id'] + 1\n",
    "\n",
    "dim_location = all_locations[['location_id', 'location']]\n",
    "print(\"\\ndim_location:\")\n",
    "print(dim_location.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.3 Additional Dimensions for Accident Data\n",
    "#\n",
    "# Create dimensions for vehicle type, weather condition, and road condition.\n",
    "\n",
    "# %% [code]\n",
    "# Vehicle Dimension\n",
    "dim_vehicle = df_accident[['vehicle_type']].drop_duplicates().reset_index(drop=True)\n",
    "dim_vehicle.reset_index(inplace=True)\n",
    "dim_vehicle.rename(columns={'index': 'vehicle_id'}, inplace=True)\n",
    "dim_vehicle['vehicle_id'] = dim_vehicle['vehicle_id'] + 1\n",
    "\n",
    "dim_vehicle = dim_vehicle[['vehicle_id', 'vehicle_type']]\n",
    "print(\"\\ndim_vehicle:\")\n",
    "print(dim_vehicle.head())\n",
    "\n",
    "# Weather Dimension\n",
    "dim_weather = df_accident[['weather_condition']].drop_duplicates().reset_index(drop=True)\n",
    "dim_weather.reset_index(inplace=True)\n",
    "dim_weather.rename(columns={'index': 'weather_id'}, inplace=True)\n",
    "dim_weather['weather_id'] = dim_weather['weather_id'] + 1\n",
    "\n",
    "dim_weather = dim_weather[['weather_id', 'weather_condition']]\n",
    "print(\"\\ndim_weather:\")\n",
    "print(dim_weather.head())\n",
    "\n",
    "# Road Condition Dimension\n",
    "dim_road = df_accident[['road_condition']].drop_duplicates().reset_index(drop=True)\n",
    "dim_road.reset_index(inplace=True)\n",
    "dim_road.rename(columns={'index': 'road_id'}, inplace=True)\n",
    "dim_road['road_id'] = dim_road['road_id'] + 1\n",
    "\n",
    "dim_road = dim_road[['road_id', 'road_condition']]\n",
    "print(\"\\ndim_road:\")\n",
    "print(dim_road.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Build Fact Tables\n",
    "#\n",
    "# ### 4.1 fact_traffic\n",
    "#\n",
    "# - Merge sensor data with `dim_time` on `date_time`.\n",
    "# - Merge sensor data with `dim_location` on `location`.\n",
    "# - Keep only the surrogate keys (`fk_time_id`, `fk_location_id`), the primary key (`sensor_id`), and the relevant numeric or measure columns.\n",
    "\n",
    "# %% [code]\n",
    "# Merge with dim_time to get fk_time_id\n",
    "fact_traffic = df_sensor.merge(dim_time[['time_id', 'date_time']], on='date_time', how='left')\n",
    "fact_traffic.rename(columns={'time_id': 'fk_time_id'}, inplace=True)\n",
    "\n",
    "# Merge with dim_location to get fk_location_id\n",
    "fact_traffic = fact_traffic.merge(dim_location, on='location', how='left')\n",
    "fact_traffic.rename(columns={'location_id': 'fk_location_id'}, inplace=True)\n",
    "\n",
    "# Select only relevant columns\n",
    "fact_traffic = fact_traffic[['sensor_id', 'fk_time_id', 'fk_location_id',\n",
    "                             'vehicle_count', 'average_speed', 'congestion_level']]\n",
    "\n",
    "print(\"\\nfact_traffic:\")\n",
    "print(fact_traffic.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.2 fact_accident\n",
    "#\n",
    "# - Merge accident data with `dim_time`, `dim_location`, `dim_vehicle`, `dim_weather`, and `dim_road`.\n",
    "# - Keep only the surrogate keys (e.g., `fk_time_id`, `fk_location_id`, etc.), the primary key (`accident_id`), and measure columns.\n",
    "\n",
    "# %% [code]\n",
    "# Merge with dim_time\n",
    "fact_accident = df_accident.merge(dim_time[['time_id', 'date_time']], on='date_time', how='left')\n",
    "fact_accident.rename(columns={'time_id': 'fk_time_id'}, inplace=True)\n",
    "\n",
    "# Merge with dim_location\n",
    "fact_accident = fact_accident.merge(dim_location, on='location', how='left')\n",
    "fact_accident.rename(columns={'location_id': 'fk_location_id'}, inplace=True)\n",
    "\n",
    "# Merge with dim_vehicle\n",
    "dim_vehicle_renamed = dim_vehicle.rename(columns={'vehicle_id': 'fk_vehicle_id'})\n",
    "fact_accident = fact_accident.merge(dim_vehicle_renamed, on='vehicle_type', how='left')\n",
    "\n",
    "# Merge with dim_weather\n",
    "dim_weather_renamed = dim_weather.rename(columns={'weather_id': 'fk_weather_id'})\n",
    "fact_accident = fact_accident.merge(dim_weather_renamed, on='weather_condition', how='left')\n",
    "\n",
    "# Merge with dim_road\n",
    "dim_road_renamed = dim_road.rename(columns={'road_id': 'fk_road_id'})\n",
    "fact_accident = fact_accident.merge(dim_road_renamed, on='road_condition', how='left')\n",
    "\n",
    "# Select only necessary columns (no redundancy)\n",
    "fact_accident = fact_accident[['accident_id', 'fk_time_id', 'fk_location_id',\n",
    "                               'fk_vehicle_id', 'fk_weather_id', 'fk_road_id',\n",
    "                               'accident_severity', 'number_of_vehicles', 'casualties',\n",
    "                               'traffic_density']]\n",
    "\n",
    "print(\"\\nfact_accident:\")\n",
    "print(fact_accident.head())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Load Tables into MySQL\n",
    "#\n",
    "# Using SQLAlchemy, we connect to the MySQL database and load each dimension and fact table.\n",
    "# All table names are in lower-case.\n",
    "\n",
    "# %% [code]\n",
    "# MySQL connection parameters\n",
    "username = 'root'\n",
    "password = '12345'\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "database = 'case6'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# Load dimension and fact tables to MySQL (if_exists='replace' will overwrite existing tables)\n",
    "dim_time.to_sql('dim_time', con=engine, index=False, if_exists='replace')\n",
    "dim_location.to_sql('dim_location', con=engine, index=False, if_exists='replace')\n",
    "dim_vehicle.to_sql('dim_vehicle', con=engine, index=False, if_exists='replace')\n",
    "dim_weather.to_sql('dim_weather', con=engine, index=False, if_exists='replace')\n",
    "dim_road.to_sql('dim_road', con=engine, index=False, if_exists='replace')\n",
    "fact_traffic.to_sql('fact_traffic', con=engine, index=False, if_exists='replace')\n",
    "fact_accident.to_sql('fact_accident', con=engine, index=False, if_exists='replace')\n",
    "\n",
    "print(\"\\nAll tables have been loaded successfully into MySQL!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
