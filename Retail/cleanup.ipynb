{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python’s chardet library for guessing csv file encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\sayan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\entfa-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'ISO-8859-1', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# Read a portion or the entire file in binary mode\n",
    "with open('AdventureWorks_Customers.csv', 'rb') as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Detect the encoding\n",
    "result = chardet.detect(raw_data)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:29: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:44: SyntaxWarning: invalid escape sequence '\\$'\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:29: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  sales_2015_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2015.csv\", encoding=\"ISO-8859-1\")\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:30: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  sales_2016_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2016.csv\", encoding=\"ISO-8859-1\")\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:31: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  sales_2017_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2017.csv\", encoding=\"ISO-8859-1\")\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:44: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  {'\\$': '', ',': ''}, regex=True).str.strip()\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:40: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  customers_df['BirthDate'] = pd.to_datetime(customers_df['BirthDate'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:103: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  sales_df['OrderDate'] = pd.to_datetime(sales_df['OrderDate'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp\\ipykernel_31580\\453496930.py:104: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  sales_df['StockDate'] = pd.to_datetime(sales_df['StockDate'], infer_datetime_format=True, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey unique: True\n",
      "ProductKey unique: True\n",
      "Composite Sales Key unique: True\n",
      "Data loaded to MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Sales & Revenue Analysis ETL Notebook\n",
    "# This notebook extracts data from CSV files, cleans and transforms it, builds a star schema, and loads the data into MySQL.\n",
    "# \n",
    "# **Datasets:**\n",
    "# - AdventureWorks_Customers.csv\n",
    "# - AdventureWorks_Products.csv\n",
    "# - AdventureWorks_Sales_2015.csv, AdventureWorks_Sales_2016.csv, AdventureWorks_Sales_2017.csv\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Import Libraries\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Read the CSV Files\n",
    "# Adjust file paths if necessary.\n",
    "\n",
    "# %%\n",
    "# Read Customer and Product data using the specified encoding\n",
    "customers_df = pd.read_csv(\"AdventureWorks_Customers.csv\", encoding=\"ISO-8859-1\")\n",
    "products_df = pd.read_csv(\"AdventureWorks_Products.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Read Sales data for multiple years with the same encoding\n",
    "sales_2015_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2015.csv\", encoding=\"ISO-8859-1\")\n",
    "sales_2016_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2016.csv\", encoding=\"ISO-8859-1\")\n",
    "sales_2017_df = pd.read_csv(\"AW Sales\\AdventureWorks_Sales_2017.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Cleaning & Transformation – Customers\n",
    "# - Convert BirthDate to datetime (with multiple formats)\n",
    "# - Clean AnnualIncome: Remove '$', commas, extra spaces then convert to numeric\n",
    "\n",
    "# %%\n",
    "# Clean BirthDate column\n",
    "customers_df['BirthDate'] = pd.to_datetime(customers_df['BirthDate'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Clean AnnualIncome: remove '$', commas and extra spaces then convert to numeric\n",
    "customers_df['AnnualIncome'] = customers_df['AnnualIncome'].replace(\n",
    "    {'\\$': '', ',': ''}, regex=True).str.strip()\n",
    "customers_df['AnnualIncome'] = pd.to_numeric(customers_df['AnnualIncome'], errors='coerce')\n",
    "\n",
    "# Check primary key uniqueness for CustomerKey\n",
    "print(\"CustomerKey unique:\", customers_df['CustomerKey'].is_unique)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Data Cleaning & Transformation – Products\n",
    "# - Transform the ProductSize column so that letter sizes (S, M, L, XL) become numeric.\n",
    "#   Mapping: S → 44, M → 48, L → 52, XL → 62. Other values (like 0) are left as is.\n",
    "# - Ensure numeric columns (ProductCost, ProductPrice) are in proper format.\n",
    "\n",
    "# %%\n",
    "# Define mapping for letter sizes\n",
    "size_mapping = {\n",
    "    'S': 44,\n",
    "    'M': 48,\n",
    "    'L': 52,\n",
    "    'XL': 62\n",
    "}\n",
    "\n",
    "def transform_size(x):\n",
    "    \"\"\"\n",
    "    Convert product size to numeric:\n",
    "    - If x is one of the letter sizes (S, M, L, XL), map it to its numeric value.\n",
    "    - If x is already numeric (or a string representing a number), return as integer.\n",
    "    - Otherwise, return np.nan.\n",
    "    \"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()  # remove extra spaces\n",
    "        if x in size_mapping:\n",
    "            return size_mapping[x]\n",
    "        else:\n",
    "            try:\n",
    "                return int(x)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Apply transformation to ProductSize\n",
    "products_df['ProductSize'] = products_df['ProductSize'].apply(transform_size)\n",
    "products_df['ProductSize'] = pd.to_numeric(products_df['ProductSize'], errors='coerce')\n",
    "\n",
    "# Check primary key uniqueness for ProductKey\n",
    "print(\"ProductKey unique:\", products_df['ProductKey'].is_unique)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Data Cleaning & Transformation – Sales Data\n",
    "# - Combine the sales data from 2015, 2016, and 2017.\n",
    "# - Convert OrderDate and StockDate to datetime.\n",
    "# - Check if the combination of OrderNumber and OrderLineItem is unique.\n",
    "# - (Later, we will calculate revenue by joining with the Product dimension.)\n",
    "\n",
    "# %%\n",
    "# Concatenate sales data\n",
    "sales_df = pd.concat([sales_2015_df, sales_2016_df, sales_2017_df], ignore_index=True)\n",
    "\n",
    "# Convert date columns to datetime\n",
    "sales_df['OrderDate'] = pd.to_datetime(sales_df['OrderDate'], infer_datetime_format=True, errors='coerce')\n",
    "sales_df['StockDate'] = pd.to_datetime(sales_df['StockDate'], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Check uniqueness of the composite key (OrderNumber + OrderLineItem)\n",
    "sales_df['CompositeKey'] = sales_df['OrderNumber'].astype(str) + \"_\" + sales_df['OrderLineItem'].astype(str)\n",
    "print(\"Composite Sales Key unique:\", sales_df['CompositeKey'].is_unique)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Handle Missing Values\n",
    "# For this example, we drop rows with missing critical values. In a production system, you might impute missing values as needed.\n",
    "\n",
    "# %%\n",
    "customers_df = customers_df.dropna(subset=['CustomerKey', 'BirthDate', 'AnnualIncome'])\n",
    "products_df = products_df.dropna(subset=['ProductKey', 'ProductPrice'])\n",
    "sales_df = sales_df.dropna(subset=['OrderDate', 'ProductKey', 'CustomerKey', 'OrderQuantity'])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Data Transformation – Derived Columns and Revenue Calculation\n",
    "# - In the fact table, we calculate revenue by joining sales with product price:\n",
    "#   Revenue = OrderQuantity * ProductPrice\n",
    "# - Also add a normalized date column for joining with the Date dimension.\n",
    "\n",
    "# %%\n",
    "# Add a normalized date column (strip time from OrderDate)\n",
    "sales_df['OrderDateNorm'] = sales_df['OrderDate'].dt.normalize()\n",
    "\n",
    "# Join sales with products to get ProductPrice and calculate revenue\n",
    "sales_df = sales_df.merge(products_df[['ProductKey', 'ProductPrice']], on='ProductKey', how='left')\n",
    "sales_df['Revenue'] = sales_df['OrderQuantity'] * sales_df['ProductPrice']\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Create Dimension Tables for Star Schema\n",
    "# - **Customer Dimension:** Based on customers_df.\n",
    "# - **Product Dimension:** Based on products_df.\n",
    "# - **Date Dimension:** Generated from the range of OrderDate values in sales_df.\n",
    "\n",
    "# %%\n",
    "# Customer Dimension\n",
    "dim_customer = customers_df.drop_duplicates(subset=['CustomerKey'])\n",
    "\n",
    "# Product Dimension\n",
    "dim_product = products_df.drop_duplicates(subset=['ProductKey'])\n",
    "\n",
    "# Date Dimension: Create a date table covering the range of OrderDateNorm\n",
    "min_date = sales_df['OrderDateNorm'].min().date()\n",
    "max_date = sales_df['OrderDateNorm'].max().date()\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    "dim_date = pd.DataFrame({'Date': date_range})\n",
    "dim_date['Year'] = dim_date['Date'].dt.year\n",
    "dim_date['Month'] = dim_date['Date'].dt.month\n",
    "dim_date['Day'] = dim_date['Date'].dt.day\n",
    "dim_date['Weekday'] = dim_date['Date'].dt.day_name()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Create the Fact Table – Sales\n",
    "# We join sales with the customer and product dimensions.\n",
    "# For simplicity, we include key columns and the calculated revenue.\n",
    "# (The composite key of OrderNumber and OrderLineItem can serve as a unique identifier.)\n",
    "\n",
    "# %%\n",
    "# Merge sales with customers and products (if needed for additional attributes)\n",
    "fact_sales = sales_df.merge(dim_customer[['CustomerKey']], on='CustomerKey', how='left') \\\n",
    "                     .merge(dim_product[['ProductKey']], on='ProductKey', how='left')\n",
    "\n",
    "# Select and reorder columns for the fact table\n",
    "fact_sales = fact_sales[['CompositeKey', 'OrderDate', 'StockDate', 'CustomerKey', 'ProductKey', \n",
    "                           'OrderQuantity', 'ProductPrice', 'Revenue']]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. Load Cleaned Data to MySQL\n",
    "# Using SQLAlchemy, we load the dimension and fact tables into a MySQL database.\n",
    "# Adjust the connection details if needed.\n",
    "\n",
    "# %%\n",
    "# MySQL connection details\n",
    "username = 'root'\n",
    "password = '12345'\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "database = 'case5'\n",
    "engine = create_engine(f'mysql+pymysql://{username}:{password}@{host}:{port}/{database}')\n",
    "\n",
    "# Load tables to MySQL (table names are in lower-case)\n",
    "dim_customer.to_sql('dim_customer', engine, index=False, if_exists='replace')\n",
    "dim_product.to_sql('dim_product', engine, index=False, if_exists='replace')\n",
    "dim_date.to_sql('dim_date', engine, index=False, if_exists='replace')\n",
    "fact_sales.to_sql('fact_sales', engine, index=False, if_exists='replace')\n",
    "\n",
    "print(\"Data loaded to MySQL successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
